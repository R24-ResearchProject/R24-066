{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\cheha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\cheha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\cheha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\cheha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\cheha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cheha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '../Dataset/train'\n",
    "validation_data_dir = '../Dataset/validation'\n",
    "test_data_dir = '../Dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data generators with preprocessing and augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data generator for validation and testing (only rescaling)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 593 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow training images in batches using data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # for binary classification\n",
    "    shuffle=True  # Shuffle the data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow validation images in batches using data generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # for binary classification\n",
    "    shuffle=False  # No need to shuffle for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X and y data from the generator\n",
    "X_train, y_train = train_generator.next()  # Assuming you are using a generator\n",
    "\n",
    "# Split the data into defect and non-defect classes\n",
    "defect_indices = np.where(y_train == 1)[0]\n",
    "non_defect_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "# Oversample the defect class manually by duplicating samples\n",
    "oversampled_defect_indices = np.random.choice(defect_indices, size=len(non_defect_indices), replace=True)\n",
    "oversampled_indices = np.concatenate((oversampled_defect_indices, non_defect_indices))\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.shuffle(oversampled_indices)\n",
    "\n",
    "# Get the oversampled data\n",
    "X_train_oversampled = X_train[oversampled_indices]\n",
    "y_train_oversampled = y_train[oversampled_indices]\n",
    "\n",
    "# Reshape X_train_oversampled back to images\n",
    "X_train_oversampled_images = X_train_oversampled.reshape(-1, 224, 224, 3)\n",
    "\n",
    "# Re-create the generator with oversampled data\n",
    "oversampled_train_generator = train_datagen.flow(X_train_oversampled_images, y_train_oversampled, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',  # for binary classification\n",
    "    shuffle=False  # No need to shuffle for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification (defect or non-defect)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 16s 11s/step - loss: 3.1567 - accuracy: 0.5625 - val_loss: 0.8420 - val_accuracy: 0.6622\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 19s 14s/step - loss: 1.5043 - accuracy: 0.4062 - val_loss: 0.9187 - val_accuracy: 0.3378\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 18s 13s/step - loss: 0.8232 - accuracy: 0.5781 - val_loss: 0.6420 - val_accuracy: 0.6622\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 18s 13s/step - loss: 0.5663 - accuracy: 0.7812 - val_loss: 0.6430 - val_accuracy: 0.6622\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 19s 15s/step - loss: 0.6317 - accuracy: 0.6719 - val_loss: 0.6579 - val_accuracy: 0.6622\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 17s 12s/step - loss: 0.6863 - accuracy: 0.6094 - val_loss: 0.7065 - val_accuracy: 0.6216\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 17s 13s/step - loss: 0.5985 - accuracy: 0.7500 - val_loss: 0.7251 - val_accuracy: 0.6757\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 18s 13s/step - loss: 0.5688 - accuracy: 0.7812 - val_loss: 0.7389 - val_accuracy: 0.6622\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 18s 14s/step - loss: 0.6202 - accuracy: 0.6406 - val_loss: 0.7461 - val_accuracy: 0.6622\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 15s 13s/step - loss: 0.6326 - accuracy: 0.6327 - val_loss: 0.8849 - val_accuracy: 0.2162\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(oversampled_train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 5s 1s/step - loss: 0.8744 - accuracy: 0.2535\n",
      "Test Loss: 0.8743811249732971, Test Accuracy: 0.2535211145877838\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Prediction: Non-defect\n",
      "Probability: 0.5493984222412109\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = '../Dataset/test/Open Seam defect dataset/2024_04_02_11_17_IMG_8773.JPG'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array /= 255.  # Normalize pixel values\n",
    "\n",
    "# Make predictions\n",
    "prediction = model.predict(img_array)\n",
    "prediction_label = 'Open-Defect' if prediction >= 0.5 else 'Non-defect'\n",
    "prediction_prob = prediction[0][0] if prediction_label == 'Open-Defect' else 1 - prediction[0][0]\n",
    "\n",
    "print(f'Prediction: {prediction_label}')\n",
    "print(f'Probability: {prediction_prob}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
